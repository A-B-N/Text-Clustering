{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Clustering using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all necessary libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read then given csr matrix in its exact format\n",
    "def csr_read(fname, ftype=\"csr\", nidx=1):\n",
    "    r\"\"\" \n",
    "        Read CSR matrix from a text file. \n",
    "        \n",
    "        \\param fname File name for CSR/CLU matrix\n",
    "        \\param ftype Input format. Acceptable formats are:\n",
    "            - csr - Compressed sparse row\n",
    "            - clu - Cluto format, i.e., CSR + header row with \"nrows ncols nnz\"\n",
    "        \\param nidx Indexing type in CSR file. What does numbering of feature IDs start with?\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    if ftype == \"clu\":\n",
    "        p = lines[0].split()\n",
    "        nrows = int(p[0])\n",
    "        ncols = int(p[1])\n",
    "        nnz = long(p[2])\n",
    "        lines = lines[1:]\n",
    "        assert(len(lines) == nrows)\n",
    "    elif ftype == \"csr\":\n",
    "        nrows = len(lines)\n",
    "        ncols = 0 \n",
    "        nnz = 0 \n",
    "        for i in range(nrows):\n",
    "            p = lines[i].split()\n",
    "            if len(p) % 2 != 0:\n",
    "                raise ValueError(\"Invalid CSR matrix. Row %d contains %d numbers.\" % (i, len(p)))\n",
    "            nnz += len(p)/2\n",
    "            for j in range(0, len(p), 2): \n",
    "                cid = int(p[j]) - nidx\n",
    "                if cid+1 > ncols:\n",
    "                    ncols = cid+1\n",
    "    else:\n",
    "        raise ValueError(\"Invalid sparse matrix ftype '%s'.\" % ftype)\n",
    "    val = np.zeros(int(nnz), dtype=np.float)\n",
    "    ind = np.zeros(int(nnz), dtype=np.int)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.long)\n",
    "    n = 0 \n",
    "    for i in range(nrows):\n",
    "        p = lines[i].split()\n",
    "        for j in range(0, len(p), 2): \n",
    "            ind[n] = int(p[j]) - nidx\n",
    "            val[n] = float(p[j+1])\n",
    "            n += 1\n",
    "        ptr[i+1] = n \n",
    "    \n",
    "    assert(n == nnz)\n",
    "    \n",
    "    return csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_idf(matrix, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        matrix = matrix.copy()\n",
    "    nrows = matrix.shape[0]\n",
    "    nnz = matrix.nnz\n",
    "    ind, val, ptr = matrix.indices, matrix.data, matrix.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_l2normalize(matrix, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        matrix = matrix.copy()\n",
    "    nrows = matrix.shape[0]\n",
    "    nnz = matrix.nnz\n",
    "    ind, val, ptr = matrix.indices, matrix.data, matrix.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = float(1.0/np.sqrt(rsum))\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CSR matrix from the input file\n",
    "csrMatrix = csr_read('train.dat', ftype=\"csr\", nidx=1)\n",
    "\n",
    "#Scale the CSR matrix by idf (Inverse Document Frequency)\n",
    "csrIDF = csr_idf(csrMatrix, copy=True)\n",
    "\n",
    "#Normalize the rows of a CSR matrix by their L-2 norm.\n",
    "csrL2Normalized = csr_l2normalize(csrIDF, copy=True)\n",
    "\n",
    "#Obtain a dense ndarray representation of the CSR matrix.\n",
    "denseMatrix = csrL2Normalized.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means to get necessary clusters for DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n",
       "        init_size=None, max_iter=100, max_no_improvement=10,\n",
       "        n_clusters=200, n_init=3, random_state=0, reassignment_ratio=0.01,\n",
       "        tol=0.0, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=200,random_state = 0)\n",
    "kmeans.fit(csrL2Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = kmeans.labels_\n",
    "points =centroids= centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers.shape, label.shape\n",
    "indices = np.asarray(list(range(0,8580)))\n",
    "lab = np.column_stack([indices,label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN Function that uses to clusters from K-Means to perform clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyDBSCAN(points, eps,minPts):\n",
    "    neighborhoods = []\n",
    "    core = []\n",
    "    border = []\n",
    "    noise = []\n",
    "\n",
    "    for i in range(len(points)):\n",
    "        neighbors = []\n",
    "        for p in range(0, len(points)):\n",
    "            # If the distance is below eps, p is a neighbor\n",
    "            if sp.spatial.distance.cosine(points[i] ,points[p]) <= eps:\n",
    "                neighbors.append(p)\n",
    "        neighborhoods.append(neighbors)\n",
    "        # If neighborhood has at least minPts, i is a core point\n",
    "        if len(neighbors) >= minPts :\n",
    "            core.append(i)\n",
    "    # Find border points \n",
    "    for i in range(len(points)):\n",
    "        neighbors = neighborhoods[i]\n",
    "        # Look at points that are not core points\n",
    "        if len(neighbors) < minPts:\n",
    "            for j in range(len(neighbors)):\n",
    "                # If one of its neighbors is a core, it is also in the core point's neighborhood, \n",
    "                # thus it is a border point rather than a noise point\n",
    "                if neighbors[j] in core:\n",
    "                    border.append(i)\n",
    "                    # Need at least one core point...\n",
    "                    break\n",
    "    # Find noise points\n",
    "    for i in range(len(points)):\n",
    "        if i not in core and i not in border:\n",
    "            noise.append(i)\n",
    "            \n",
    "    # # Invoke graph instance to visualize the cluster\n",
    "    G = nx.Graph()\n",
    "    nodes = core\n",
    "    G.add_nodes_from(nodes)\n",
    "    # Create neighborhood\n",
    "    for i in range(len(nodes)):\n",
    "        for p in range(len(nodes)):\n",
    "            # If the distance is below the threshold, add a link in the graph.\n",
    "            if p != i and sp.spatial.distance.cosine(points[nodes[i]] ,points[nodes[p]]) <= eps:\n",
    "                G.add_edges_from([(nodes[i], nodes[p])])\n",
    "    # List the connected components / clusters\n",
    "    clusters = list(nx.connected_components(G))\n",
    "    print(\"# clusters:\", len(clusters))\n",
    "    print(\"clusters: \", clusters)\n",
    "    centers = []\n",
    "    for cluster in clusters:\n",
    "        coords = []\n",
    "        for point in list(cluster):\n",
    "            coords.append(points[point])\n",
    "        center = np.mean(coords,axis =0)\n",
    "        centers.append(center)\n",
    "    expanded_clusters = clusters\n",
    "    for pt in border:\n",
    "        distances = {}\n",
    "        for i, center in enumerate(centers):\n",
    "    #         print(\"point = \", pt, \" center = \", i)\n",
    "    #         print(scipy.spatial.distance.cosine(points[pt],center))\n",
    "            distances[i] = sp.spatial.distance.cosine(points[pt],center)\n",
    "    #     distances = \n",
    "    #     print(\"closest cluster for point %d = %d \" %(pt, min(distances, key=distances.get)))\n",
    "        closest_cluster = min(distances, key=distances.get)\n",
    "        expanded_clusters[closest_cluster].add(pt)\n",
    "    #     print(clusters[closest_cluster])\n",
    "    label , centroids, expanded_clusters\n",
    "    centroid_labels = [len(clusters)+1]* len(centroids)\n",
    "    for index, clstr in enumerate(expanded_clusters):\n",
    "        for n in clstr:\n",
    "            centroid_labels[n]= index\n",
    "    print(np.unique(centroid_labels))\n",
    "    final_labels = [0]*len(label)\n",
    "    for i,l in enumerate(label):\n",
    "        final_labels[i] = centroid_labels[l]\n",
    "    np.unique(final_labels)\n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# clusters: 2\n",
      "clusters:  [{52}, {123}]\n",
      "[0 1 3]\n"
     ]
    }
   ],
   "source": [
    "# The number of clusters vary depending upon the eps and minPts you pass\n",
    "eps =0.53\n",
    "minPts = 7\n",
    "d = MyDBSCAN(points, eps,minPts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.004\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % silhouette_score(csrL2Normalized, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prediction78.dat\", \"w\") as f:\n",
    "            for l in d:\n",
    "                f.write(\"%s\\n\"%l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
